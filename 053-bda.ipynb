{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\ntry:\n    spark = SparkSession.builder \\\n        .appName(\"ClassificationModel\") \\\n        .getOrCreate()\n    spark.sparkContext.setLogLevel(\"ERROR\")\n    print(\"Spark session initialized successfully.\")\nexcept Exception as e:\n    print(f\"Error initializing Spark session: {e}\")\n    exit(1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:19:45.816635Z","iopub.execute_input":"2025-04-25T15:19:45.816930Z","iopub.status.idle":"2025-04-25T15:19:52.247538Z","shell.execute_reply.started":"2025-04-25T15:19:45.816910Z","shell.execute_reply":"2025-04-25T15:19:52.246543Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/25 15:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"name":"stdout","text":"Spark session initialized successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\niris_df['label'] = iris.target\n\ndf = spark.createDataFrame(iris_df)\ndf.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:19:55.345779Z","iopub.execute_input":"2025-04-25T15:19:55.346468Z","iopub.status.idle":"2025-04-25T15:20:02.435827Z","shell.execute_reply.started":"2025-04-25T15:19:55.346432Z","shell.execute_reply":"2025-04-25T15:20:02.434724Z"}},"outputs":[{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+-----------------+----------------+-----------------+----------------+-----+\n|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|label|\n+-----------------+----------------+-----------------+----------------+-----+\n|              5.1|             3.5|              1.4|             0.2|    0|\n|              4.9|             3.0|              1.4|             0.2|    0|\n|              4.7|             3.2|              1.3|             0.2|    0|\n|              4.6|             3.1|              1.5|             0.2|    0|\n|              5.0|             3.6|              1.4|             0.2|    0|\n+-----------------+----------------+-----------------+----------------+-----+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler, StringIndexer\n\nfeature_cols = iris.feature_names\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\ndf_assembled = assembler.transform(df).select(\"features\", \"label\")\ndf_assembled.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:20:28.148211Z","iopub.execute_input":"2025-04-25T15:20:28.148596Z","iopub.status.idle":"2025-04-25T15:20:29.532006Z","shell.execute_reply.started":"2025-04-25T15:20:28.148569Z","shell.execute_reply":"2025-04-25T15:20:29.530974Z"}},"outputs":[{"name":"stdout","text":"+-----------------+-----+\n|         features|label|\n+-----------------+-----+\n|[5.1,3.5,1.4,0.2]|    0|\n|[4.9,3.0,1.4,0.2]|    0|\n|[4.7,3.2,1.3,0.2]|    0|\n|[4.6,3.1,1.5,0.2]|    0|\n|[5.0,3.6,1.4,0.2]|    0|\n+-----------------+-----+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:21:14.317845Z","iopub.execute_input":"2025-04-25T15:21:14.318450Z","iopub.status.idle":"2025-04-25T15:21:14.333464Z","shell.execute_reply.started":"2025-04-25T15:21:14.318426Z","shell.execute_reply":"2025-04-25T15:21:14.332605Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(featuresCol='features', labelCol='label')\nlr_model = lr.fit(train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:21:16.889461Z","iopub.execute_input":"2025-04-25T15:21:16.890071Z","iopub.status.idle":"2025-04-25T15:21:20.761692Z","shell.execute_reply.started":"2025-04-25T15:21:16.890048Z","shell.execute_reply":"2025-04-25T15:21:20.760672Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"predictions = lr_model.transform(test_data)\npredictions.select(\"features\", \"label\", \"prediction\").show(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:21:32.756525Z","iopub.execute_input":"2025-04-25T15:21:32.756843Z","iopub.status.idle":"2025-04-25T15:21:33.458975Z","shell.execute_reply.started":"2025-04-25T15:21:32.756822Z","shell.execute_reply":"2025-04-25T15:21:33.457978Z"}},"outputs":[{"name":"stdout","text":"+-----------------+-----+----------+\n|         features|label|prediction|\n+-----------------+-----+----------+\n|[4.6,3.1,1.5,0.2]|    0|       0.0|\n|[4.7,3.2,1.6,0.2]|    0|       0.0|\n|[4.8,3.1,1.6,0.2]|    0|       0.0|\n|[4.9,3.1,1.5,0.2]|    0|       0.0|\n|[5.1,3.3,1.7,0.5]|    0|       0.0|\n|[5.1,3.8,1.5,0.3]|    0|       0.0|\n|[5.4,3.7,1.5,0.2]|    0|       0.0|\n|[5.7,4.4,1.5,0.4]|    0|       0.0|\n|[4.4,3.0,1.3,0.2]|    0|       0.0|\n|[4.9,2.4,3.3,1.0]|    1|       1.0|\n|[4.9,3.6,1.4,0.1]|    0|       0.0|\n|[5.2,2.7,3.9,1.4]|    1|       1.0|\n|[5.6,2.9,3.6,1.3]|    1|       1.0|\n|[5.8,2.7,4.1,1.0]|    1|       1.0|\n|[6.5,2.8,4.6,1.5]|    1|       1.0|\n|[6.9,3.1,4.9,1.5]|    1|       1.0|\n|[5.0,2.3,3.3,1.0]|    1|       1.0|\n|[5.5,2.6,4.4,1.2]|    1|       1.0|\n|[5.6,3.0,4.1,1.3]|    1|       1.0|\n|[5.7,2.6,3.5,1.0]|    1|       1.0|\n+-----------------+-----+----------+\nonly showing top 20 rows\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:21:37.869300Z","iopub.execute_input":"2025-04-25T15:21:37.869818Z","iopub.status.idle":"2025-04-25T15:21:38.594756Z","shell.execute_reply.started":"2025-04-25T15:21:37.869796Z","shell.execute_reply":"2025-04-25T15:21:38.593980Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 1.0000\n","output_type":"stream"}],"execution_count":12}]}